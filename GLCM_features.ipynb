{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib import axes\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "GS —> GLCM (mean) \\\n",
    "&ensp; Number of color levels: size of (square) matrix N \\\n",
    "&ensp; Enlist GLCMs of all directions (this can help when tackling rotation invariance)\n",
    "\n",
    "GLCM —> Normed GLCM \\\n",
    "Normalize GLCM.\n",
    "ALL first order statistics needed for computing texture features\n",
    "* mu_x\n",
    "* mu_y\n",
    "* sigma_x\n",
    "* sigma_y\n",
    "* ...\n",
    "\n",
    "normed GLCM —> All features \\\n",
    "Compute and Enlist features (Contrast, Energy, Homogeneity, Correlation, Autocorrelation, Cluster prominence, Cluster shade, Difference entropy, Difference variance, Dissimilarity, Entropy, Information measure of correlation 1, Information measure of correlation 2, Inverse difference, Sum average, Sum entropy, Sum of squares, Sum variance) from (Löfstedt et al. 2019).\n",
    "\n",
    "**Following structure of step 4 in the paper with a Psi component and a Phi component for the function of each feature.**\n",
    "\n",
    "Refference: Tommy Löfstedt, Patrik Brynolfsson , Thomas Asklund, Tufve Nyholm, Anders Garpebring. \"Gray-level invariant Haralick texture features\" (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE: Image to Grayscale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1: Grayscale to GLCM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_glcms(gs_image, levels = 2):\n",
    "\n",
    "    glcm0 = graycomatrix(gs_image, distances=[1], angles=[0], levels=levels, symmetric=False)\n",
    "    glcm45 = graycomatrix(gs_image, distances=[1], angles=[np.pi/4], levels=levels, symmetric=False)\n",
    "    glcm90 = graycomatrix(gs_image, distances=[1], angles=[np.pi/2], levels=levels, symmetric=False)\n",
    "    glcm135 = graycomatrix(gs_image, distances=[1], angles=[3 * np.pi/4], levels=levels, symmetric=False)\n",
    "    mean_glcm = (glcm0 + glcm45 + glcm90 + glcm135)/4\n",
    "\n",
    "    glcms = {'mean': mean_glcm, '0': glcm0, '45': glcm45, '90': glcm90, '135': glcm135}\n",
    "\n",
    "    return glcms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2: Normalize GLCM and Compute First-Order Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_feature_statistics(mean_glcm):\n",
    "\n",
    "    normed_glcm = mean_glcm / np.sum(mean_glcm)\n",
    "    N = normed_glcm.shape[0]\n",
    "    i_indices = np.arange(N) + 1\n",
    "    j_indices = np.arange(N) + 1\n",
    "    \n",
    "    # Marginal probabilities\n",
    "    p_x = np.sum(normed_glcm, axis=1)  # Sum over j\n",
    "    p_x = p_x.squeeze()\n",
    "    p_y = np.sum(normed_glcm, axis=0)  # Sum over i\n",
    "    p_y = p_y.squeeze()\n",
    "    \n",
    "    # Mean values\n",
    "    indices = np.arange(N) + 1\n",
    "    mu_x = np.dot(indices, p_x)\n",
    "    mu_y = np.dot(indices, p_y)\n",
    "    mu = (mu_x + mu_y) / 2\n",
    "    \n",
    "    # Standard deviations\n",
    "    sigma_x = np.sqrt(np.sum(((i_indices - mu_x) ** 2) * p_x))\n",
    "    sigma_y = np.sqrt(np.sum(((j_indices - mu_y) ** 2) * p_y))\n",
    "    \n",
    "    # HX and HY\n",
    "    p_x_nonzero = p_x[p_x > 0]\n",
    "    HX = -np.sum(p_x_nonzero * np.log2(p_x_nonzero))\n",
    "    \n",
    "    p_y_nonzero = p_y[p_y > 0]\n",
    "    HY = -np.sum(p_y_nonzero * np.log2(p_y_nonzero))\n",
    "    \n",
    "    # p_x+y and p_x-y \n",
    "    p_sum = np.zeros(2 * N - 1)\n",
    "    p_diff = np.zeros(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            # p_x+y\n",
    "            p_sum[i + j] += normed_glcm[i, j]\n",
    "            # p_x-y\n",
    "            p_diff[abs(i - j)] += normed_glcm[i, j]\n",
    "    \n",
    "    stats = {\n",
    "        'Pij' : normed_glcm,\n",
    "        'N': N,\n",
    "        'mu_x': mu_x,\n",
    "        'mu_y': mu_y,\n",
    "        'mu': mu,\n",
    "        'sigma_x': sigma_x,\n",
    "        'sigma_y': sigma_y,\n",
    "        'p_x': p_x,\n",
    "        'p_y': p_y,\n",
    "        'HX': HX,\n",
    "        'HY': HY,\n",
    "        'p_sum': p_sum,\n",
    "        'p_diff': p_diff\n",
    "    }\n",
    "    \n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New features\n",
    "\n",
    "Since original functions are available in scikit-image, we will use their functions as it is more computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3: Compute Haralick Texture Features \\\n",
    "Structure: Each feature has Psi and Phi components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_autocorrelation(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "\n",
    "    autocorrelation = 0.0\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            i_1 = i + 1\n",
    "            j_1 = j + 1\n",
    "\n",
    "            # φ(i,j,g(P)) = i·j\n",
    "            phi = i_1 * j_1\n",
    "\n",
    "            # ψ(p(i,j)) = p(i,j)\n",
    "            psi = normed_glcm[i,j]\n",
    "\n",
    "            autocorrelation += phi * psi\n",
    "\n",
    "    return autocorrelation\n",
    "\n",
    "\n",
    "def compute_cluster_prominence(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    p_x = stats['p_x']\n",
    "\n",
    "    # mu = sum(i * px_i)           ---***---\n",
    "    # OR mu = mu_x + mu_y          ---***---\n",
    "    mu =  np.dot(np.arange(1,N+1), p_x)\n",
    "\n",
    "    cluster_prominence = 0.0\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            i_1 = i + 1\n",
    "            j_1 = j + 1\n",
    "            \n",
    "            # φ(i,j,g(P)) = (i + j - 2*mu)^3 \n",
    "            phi = (i_1 + j_1 - 2 * mu)**3\n",
    "\n",
    "            # ψ(p(i,j)) = p(i,j)\n",
    "            psi = normed_glcm[i,j]\n",
    "            \n",
    "            cluster_prominence += phi * psi\n",
    "            \n",
    "    return cluster_prominence\n",
    "\n",
    "\n",
    "def compute_cluster_shade(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    p_x = stats['p_x']\n",
    "\n",
    "    # mu = sum(i * px_i)           ---***---\n",
    "    # OR mu = mu_x + mu_y          ---***---\n",
    "    mu =  np.dot(np.arange(1,N+1), p_x)\n",
    "\n",
    "    cluster_shade = 0.0\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            i_1 = i + 1\n",
    "            j_1 = j + 1\n",
    "            \n",
    "            # φ(i,j,g(P)) = (i + j - 2*mu)^4 \n",
    "            phi = (i_1 + j_1 - 2 * mu)**4\n",
    "\n",
    "            # ψ(p(i,j)) = p(i,j)\n",
    "            psi = normed_glcm[i,j]\n",
    "            \n",
    "            cluster_shade += phi * psi\n",
    "    \n",
    "    return cluster_shade\n",
    "\n",
    "\n",
    "def compute_dissimilarity(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    \n",
    "    dissimilarity = 0.0\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            i_1 = i + 1\n",
    "            j_1 = j + 1\n",
    "            \n",
    "            # φ(i,j,g(P)) = |i - j|\n",
    "            phi = abs(i_1 - j_1)\n",
    "            \n",
    "            # ψ(p(i,j)) = p(i,j)\n",
    "            psi = normed_glcm[i, j] \n",
    "            \n",
    "            dissimilarity += phi * psi\n",
    "    \n",
    "    return dissimilarity\n",
    "\n",
    "\n",
    "def compute_entropy(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    \n",
    "    entropy_val = 0.0\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            p = normed_glcm[i, j]\n",
    "            if p > 0:\n",
    "                # φ(i,j,g(P)) = 1\n",
    "                phi = 1\n",
    "                \n",
    "                # ψ(p(i,j)) = -p(i,j) * log(p(i,j))\n",
    "                psi = -p * np.log(p)\n",
    "                \n",
    "                entropy_val += phi * psi\n",
    "    \n",
    "    return entropy_val\n",
    "\n",
    "def compute_difference_entropy(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    \n",
    "    # p_x-y(k)\n",
    "    p_diff = np.zeros(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            k = abs((i + 1) - (j + 1))\n",
    "            if k < N:\n",
    "                p_diff[k] += normed_glcm[i, j]\n",
    "    \n",
    "\n",
    "    diff_entropy = 0.0\n",
    "    for k in range(N):\n",
    "        if p_diff[k] > 0:\n",
    "            diff_entropy -= p_diff[k] * np.log(p_diff[k])\n",
    "    \n",
    "    return diff_entropy\n",
    "\n",
    "\n",
    "def compute_difference_variance(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    \n",
    "    # p_x-y(k)\n",
    "    p_diff = np.zeros(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            k = abs((i + 1) - (j + 1))\n",
    "            if k < N:\n",
    "                p_diff[k] += normed_glcm[i, j]\n",
    "    \n",
    "    # mean\n",
    "    k_values = np.arange(N)\n",
    "    mu_diff = np.sum(k_values * p_diff)\n",
    "    \n",
    "    # variance\n",
    "    diff_variance = 0.0\n",
    "    for k in range(N):\n",
    "        diff_variance += ((k - mu_diff) ** 2) * p_diff[k]\n",
    "    \n",
    "    return diff_variance\n",
    "\n",
    "\n",
    "def compute_inverse_difference(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    \n",
    "    inv_diff = 0.0\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            i_1based = i + 1\n",
    "            j_1based = j + 1\n",
    "            \n",
    "            # φ(i,j,g(P)) = 1\n",
    "            phi = 1\n",
    "            \n",
    "            # ψ(p(i,j)) = p(i,j) / (1 + |i-j|)\n",
    "            psi = normed_glcm[i, j] / (1 + abs(i_1based - j_1based))\n",
    "            \n",
    "            inv_diff += phi * psi\n",
    "    \n",
    "    return inv_diff\n",
    "\n",
    "\n",
    "def compute_sum_average(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "\n",
    "    # Compute p_x+y(k) for k = 2 to 2N\n",
    "    p_sum = np.zeros(2 * N + 1)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            k = (i + 1) + (j + 1)\n",
    "            if k <= 2 * N:\n",
    "                p_sum[k] += normed_glcm[i, j]\n",
    "    \n",
    "    # Compute sum average\n",
    "    sum_avg = 0.0\n",
    "    for k in range(2, 2 * N + 1):\n",
    "        sum_avg += k * p_sum[k]\n",
    "    \n",
    "    return sum_avg\n",
    "\n",
    "\n",
    "def compute_sum_entropy(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    \n",
    "    p_sum = np.zeros(2 * N + 1)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            k = (i + 1) + (j + 1)\n",
    "            if k <= 2 * N:\n",
    "                p_sum[k] += normed_glcm[i, j]\n",
    "    \n",
    "    # Compute sum entropy\n",
    "    sum_ent = 0.0\n",
    "    for k in range(2, 2 * N + 1):\n",
    "        if p_sum[k] > 0:\n",
    "            sum_ent -= p_sum[k] * np.log(p_sum[k])\n",
    "    \n",
    "    return sum_ent\n",
    "\n",
    "\n",
    "def compute_sum_of_squares(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    mu_x = stats['mu_x']\n",
    "    \n",
    "    sum_squares = 0.0\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            i_1based = i + 1\n",
    "            \n",
    "            # φ(i,j,g(P)) = (i - μ_x)^2\n",
    "            phi = (i_1based - mu_x) ** 2\n",
    "            \n",
    "            # ψ(p(i,j)) = p(i,j)\n",
    "            psi = normed_glcm[i, j]\n",
    "            \n",
    "            sum_squares += phi * psi\n",
    "    \n",
    "    return sum_squares\n",
    "\n",
    "\n",
    "def compute_sum_variance(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    \n",
    "    # Compute p_x+y(k) for k = 2 to 2N\n",
    "    p_sum = np.zeros(2 * N + 1)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            k = (i + 1) + (j + 1)\n",
    "            if k <= 2 * N:\n",
    "                p_sum[k] += normed_glcm[i, j]\n",
    "    \n",
    "    # Compute sum average\n",
    "    sum_avg = 0.0\n",
    "    for k in range(2, 2 * N + 1):\n",
    "        sum_avg += k * p_sum[k]\n",
    "    \n",
    "    # Compute sum variance\n",
    "    sum_var = 0.0\n",
    "    for k in range(2, 2 * N + 1):\n",
    "        sum_var += ((k - sum_avg) ** 2) * p_sum[k]\n",
    "    \n",
    "    return sum_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_information_measure_correlation_1(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    p_x = stats['p_x']\n",
    "    p_y = stats['p_y']\n",
    "    \n",
    "    # HXY: Entropy of GLCM\n",
    "    HXY = 0.0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if normed_glcm[i, j] > 0:\n",
    "                HXY -= normed_glcm[i, j] * np.log(normed_glcm[i, j])\n",
    "    \n",
    "    # HX: Entropy of p_x\n",
    "    HX = 0.0\n",
    "    for i in range(N):\n",
    "        if p_x[i] > 0:\n",
    "            HX -= p_x[i] * np.log(p_x[i])\n",
    "    \n",
    "    # HY: Entropy of p_y\n",
    "    HY = 0.0\n",
    "    for j in range(N):\n",
    "        if p_y[j] > 0:\n",
    "            HY -= p_y[j] * np.log(p_y[j])\n",
    "    \n",
    "    # HXY1: sum over p(i,j) * log(p_x(i) * p_y(j))\n",
    "    HXY1 = 0.0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if normed_glcm[i, j] > 0 and p_x[i] > 0 and p_y[j] > 0:\n",
    "                HXY1 -= normed_glcm[i, j] * np.log(p_x[i] * p_y[j])\n",
    "    \n",
    "    # IMC1\n",
    "    max_HX_HY = max(HX, HY)\n",
    "    if max_HX_HY > 0:\n",
    "        imc1 = (HXY - HXY1) / max_HX_HY\n",
    "    else:\n",
    "        imc1 = 0.0\n",
    "    \n",
    "    return imc1\n",
    "\n",
    "\n",
    "def compute_information_measure_correlation_2(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    p_x = stats['p_x']\n",
    "    p_y = stats['p_y']\n",
    "    \n",
    "    # HXY: Entropy of GLCM\n",
    "    HXY = 0.0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if normed_glcm[i, j] > 0:\n",
    "                HXY -= normed_glcm[i, j] * np.log(normed_glcm[i, j])\n",
    "    \n",
    "    # HXY2: sum over p_x(i) * p_y(j) * log(p_x(i) * p_y(j))\n",
    "    HXY2 = 0.0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if p_x[i] > 0 and p_y[j] > 0:\n",
    "                HXY2 -= p_x[i] * p_y[j] * np.log(p_x[i] * p_y[j])\n",
    "    \n",
    "    # IMC2\n",
    "    term = 1 - np.exp(-2 * (HXY2 - HXY))\n",
    "    imc2 = np.sqrt(max(0, term))\n",
    "    \n",
    "    return imc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## faster functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_autocorrelation_fast(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    I = np.arange(1, N + 1)\n",
    "    J = np.arange(1, N + 1)\n",
    "\n",
    "    autocorrelation = I.T @ np.squeeze(normed_glcm) @ J\n",
    "\n",
    "    return autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses numpy vectorized operations instead of python nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_indices(N):\n",
    "    # Create the matrices of (i+1) and (j+1) indices for vectorization\n",
    "    indices = np.arange(1, N + 1)\n",
    "    I = np.repeat(indices.reshape(-1, 1), N, axis=1) # Matrix of row indices (i+1)\n",
    "    J = np.repeat(indices.reshape(1, -1), N, axis=0) # Matrix of column indices (j+1)\n",
    "    return I, J\n",
    "\n",
    "\n",
    "def compute_cluster_prominence_fast(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    p_x = stats['p_x']\n",
    "    I, J = _get_indices(N)\n",
    "\n",
    "    # mu is calculated using dot product on the flattened index array and marginal probability\n",
    "    mu = np.dot(np.arange(1, N + 1), p_x)\n",
    "\n",
    "    # Cluster Prominence = sum( (i + j - 2*mu)^3 * Pij )\n",
    "    # (I + J - 2 * mu) computes the inner term for all elements simultaneously\n",
    "    phi_matrix = (I + J - 2 * mu)**3\n",
    "    cluster_prominence = np.sum(phi_matrix * normed_glcm)\n",
    "    return cluster_prominence\n",
    "\n",
    "\n",
    "def compute_cluster_shade_fast(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    p_x = stats['p_x']\n",
    "    I, J = _get_indices(N)\n",
    "\n",
    "    mu = np.dot(np.arange(1, N + 1), p_x)\n",
    "\n",
    "    # Cluster Shade = sum( (i + j - 2*mu)^4 * Pij )\n",
    "    phi_matrix = (I + J - 2 * mu)**4\n",
    "    cluster_shade = np.sum(phi_matrix * normed_glcm)\n",
    "    return cluster_shade\n",
    "\n",
    "\n",
    "def compute_dissimilarity_fast(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "    N = normed_glcm.shape[0]\n",
    "    I, J = _get_indices(N)\n",
    "\n",
    "    # Dissimilarity = sum( |i - j| * Pij )\n",
    "    # abs(I - J) computes the |i - j| term for all elements\n",
    "    dissimilarity = np.sum(np.abs(I - J) * normed_glcm)\n",
    "    return dissimilarity\n",
    "\n",
    "\n",
    "def compute_entropy_fast(stats):\n",
    "    normed_glcm = stats['Pij']\n",
    "\n",
    "    # Entropy = sum( -Pij * log(Pij) ) for Pij > 0\n",
    "    # Use boolean indexing to only consider non-zero elements\n",
    "    non_zero_P = normed_glcm[normed_glcm > 0]\n",
    "\n",
    "    # np.log is a vectorized operation\n",
    "    # The sum is over the non-zero elements only\n",
    "    entropy_val = -np.sum(non_zero_P * np.log(non_zero_P))\n",
    "    return entropy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_plot(image_in, tile_s=8, tile_o=4, N=256):\n",
    "    image_in = image_in.convert('L')\n",
    "    image_in_array = np.array(image_in)\n",
    "    row_size = image_in_array.shape[0]\n",
    "    col_size = image_in_array.shape[1]\n",
    "\n",
    "    tiles = []\n",
    "    glcms = []\n",
    "    \n",
    "    # Initialize lists for all features\n",
    "    # contrast_value = []\n",
    "    # correlation_value = []\n",
    "    # homogeneity_value = []\n",
    "    # energy_value = []\n",
    "    autocorrelation_value = []\n",
    "    # cluster_prominence_value = []\n",
    "    # cluster_shade_value = []\n",
    "    # dissimilarity_value = []\n",
    "    # entropy_value = []\n",
    "\n",
    "    for r in range(0, row_size - tile_s + 1, tile_o):\n",
    "        for c in range(0, col_size - tile_s + 1, tile_o):\n",
    "            tile = image_in_array[r:r+tile_s, c:c+tile_s]\n",
    "            tiles.append(tile)\n",
    "            distances = [1]\n",
    "\n",
    "            glcm0 = graycomatrix(tile, distances=distances, angles=[0], levels=N, symmetric=False)\n",
    "            glcm1 = graycomatrix(tile, distances=distances, angles=[np.pi/4], levels=N, symmetric=False)\n",
    "            glcm2 = graycomatrix(tile, distances=distances, angles=[np.pi/2], levels=N, symmetric=False)\n",
    "            glcm3 = graycomatrix(tile, distances=distances, angles=[3 * np.pi/4], levels=N, symmetric=False)\n",
    "\n",
    "            glcm = (glcm0 + glcm1 + glcm2 + glcm3) / 4\n",
    "            glcms.append(glcm)\n",
    "            \n",
    "            # Create stats dictionary for custom features\n",
    "            stats = pre_feature_statistics(glcm)\n",
    "\n",
    "            # Standard features\n",
    "            # contrast = float(graycoprops(glcm, 'contrast'))\n",
    "            # correlation = float(graycoprops(glcm, 'correlation'))\n",
    "            # homogeneity = float(graycoprops(glcm, 'homogeneity'))\n",
    "            # energy = float(graycoprops(glcm, 'energy'))\n",
    "            \n",
    "            # Custom features\n",
    "            autocorrelation = compute_autocorrelation_fast(stats)\n",
    "            # cluster_prominence = compute_cluster_prominence(stats)\n",
    "            # cluster_shade = compute_cluster_shade(stats)\n",
    "            # dissimilarity = compute_dissimilarity(stats)\n",
    "            # entropy = compute_entropy(stats)\n",
    "\n",
    "            # contrast_value.append(contrast)\n",
    "            # correlation_value.append(correlation)\n",
    "            # homogeneity_value.append(homogeneity)\n",
    "            # energy_value.append(energy)\n",
    "            autocorrelation_value.append(autocorrelation)\n",
    "            # cluster_prominence_value.append(cluster_prominence)\n",
    "            # cluster_shade_value.append(cluster_shade)\n",
    "            # dissimilarity_value.append(dissimilarity)\n",
    "            # entropy_value.append(entropy)\n",
    "\n",
    "    # Normalize all features using min-max\n",
    "    def normalize_feature(values):\n",
    "        val_max = max(values)\n",
    "        val_min = min(values)\n",
    "        if val_max - val_min == 0:\n",
    "            return [0.5] * len(values), val_min, val_max\n",
    "        return [(val - val_min) / (val_max - val_min) for val in values], val_min, val_max\n",
    "\n",
    "    # contrast_values, contrast_min, contrast_max = normalize_feature(contrast_value)\n",
    "    # correlation_values, correlation_min, correlation_max = normalize_feature(correlation_value)\n",
    "    # homogeneity_values, homogeneity_min, homogeneity_max = normalize_feature(homogeneity_value)\n",
    "    # energy_values, energy_min, energy_max = normalize_feature(energy_value)\n",
    "    autocorrelation_values, autocorrelation_min, autocorrelation_max = normalize_feature(autocorrelation_value)\n",
    "    # cluster_prominence_values, cluster_prominence_min, cluster_prominence_max = normalize_feature(cluster_prominence_value)\n",
    "    # cluster_shade_values, cluster_shade_min, cluster_shade_max = normalize_feature(cluster_shade_value)\n",
    "    # dissimilarity_values, dissimilarity_min, dissimilarity_max = normalize_feature(dissimilarity_value)\n",
    "    # entropy_values, entropy_min, entropy_max = normalize_feature(entropy_value)\n",
    "\n",
    "    # Reshape features into arrays\n",
    "    shape = (int(2*(row_size/tile_s) - 1), int(2*(col_size/tile_s) - 1))\n",
    "    \n",
    "    # contrast_array = np.reshape(contrast_values, shape)\n",
    "    # correlation_array = np.reshape(correlation_values, shape)\n",
    "    # homogeneity_array = np.reshape(homogeneity_values, shape)\n",
    "    # energy_array = np.reshape(energy_values, shape)\n",
    "    autocorrelation_array = np.reshape(autocorrelation_values, shape)\n",
    "    # cluster_prominence_array = np.reshape(cluster_prominence_values, shape)\n",
    "    # cluster_shade_array = np.reshape(cluster_shade_values, shape)\n",
    "    # dissimilarity_array = np.reshape(dissimilarity_values, shape)\n",
    "    # entropy_array = np.reshape(entropy_values, shape)\n",
    "\n",
    "    # Create figure with 5 rows and 2 columns (10 subplots total)\n",
    "    fig, ax = plt.subplots(5, 2, figsize=(10, 14))\n",
    "\n",
    "    # Original Image\n",
    "    original = ax[0, 0].imshow(image_in_array, cmap='gray')\n",
    "    ax[0, 0].axis('off')\n",
    "    ax[0, 0].set_title(\"Original Image\")\n",
    "    fig.colorbar(original, ax=ax[0, 0])\n",
    "\n",
    "    # Contrast\n",
    "    # contrast_img = ax[0, 1].imshow(contrast_array, cmap='Reds')\n",
    "    # ax[0, 1].axis('off')\n",
    "    # ax[0, 1].set_title(\"Contrast\")\n",
    "    # cbar = fig.colorbar(contrast_img, ax=ax[0, 1])\n",
    "    # cbar.set_ticks(ticks=[0, 1], labels=[round(contrast_min, 4), round(contrast_max, 4)])\n",
    "\n",
    "    # Homogeneity\n",
    "    # homogeneity_img = ax[1, 0].imshow(homogeneity_array, cmap='Reds')\n",
    "    # ax[1, 0].axis('off')\n",
    "    # ax[1, 0].set_title(\"Homogeneity\")\n",
    "    # cbar = fig.colorbar(homogeneity_img, ax=ax[1, 0])\n",
    "    # cbar.set_ticks(ticks=[0, 1], labels=[round(homogeneity_min, 4), round(homogeneity_max, 4)])\n",
    "\n",
    "    # Correlation\n",
    "    # correlation_img = ax[1, 1].imshow(correlation_array, cmap='Reds')\n",
    "    # ax[1, 1].axis('off')\n",
    "    # ax[1, 1].set_title(\"Correlation\")\n",
    "    # cbar = fig.colorbar(correlation_img, ax=ax[1, 1])\n",
    "    # cbar.set_ticks(ticks=[0, 1], labels=[round(correlation_min, 4), round(correlation_max, 4)])\n",
    "\n",
    "    # Energy\n",
    "    # energy_img = ax[2, 0].imshow(energy_array, cmap='Reds')\n",
    "    # ax[2, 0].axis('off')\n",
    "    # ax[2, 0].set_title(\"Energy\")\n",
    "    # cbar = fig.colorbar(energy_img, ax=ax[2, 0])\n",
    "    # cbar.set_ticks(ticks=[0, 1], labels=[round(energy_min, 4), round(energy_max, 4)])\n",
    "\n",
    "    # Autocorrelation\n",
    "    autocorrelation_img = ax[2, 1].imshow(autocorrelation_array, cmap='Reds')\n",
    "    ax[2, 1].axis('off')\n",
    "    ax[2, 1].set_title(\"Autocorrelation\")\n",
    "    cbar = fig.colorbar(autocorrelation_img, ax=ax[2, 1])\n",
    "    cbar.set_ticks(ticks=[0, 1], labels=[round(autocorrelation_min, 4), round(autocorrelation_max, 4)])\n",
    "\n",
    "    # Cluster Prominence\n",
    "    # cluster_prominence_img = ax[3, 0].imshow(cluster_prominence_array, cmap='Reds')\n",
    "    # ax[3, 0].axis('off')\n",
    "    # ax[3, 0].set_title(\"Cluster Prominence\")\n",
    "    # cbar = fig.colorbar(cluster_prominence_img, ax=ax[3, 0])\n",
    "    # cbar.set_ticks(ticks=[0, 1], labels=[f'{cluster_prominence_min:.2e}', f'{cluster_prominence_max:.2e}'])\n",
    "\n",
    "    # Cluster Shade\n",
    "    # cluster_shade_img = ax[3, 1].imshow(cluster_shade_array, cmap='Reds')\n",
    "    # ax[3, 1].axis('off')\n",
    "    # ax[3, 1].set_title(\"Cluster Shade\")\n",
    "    # cbar = fig.colorbar(cluster_shade_img, ax=ax[3, 1])\n",
    "    # cbar.set_ticks(ticks=[0, 1], labels=[f'{cluster_shade_min:.2e}', f'{cluster_shade_max:.2e}'])\n",
    "\n",
    "    # Dissimilarity\n",
    "    # dissimilarity_img = ax[4, 0].imshow(dissimilarity_array, cmap='Reds')\n",
    "    # ax[4, 0].axis('off')\n",
    "    # ax[4, 0].set_title(\"Dissimilarity\")\n",
    "    # cbar = fig.colorbar(dissimilarity_img, ax=ax[4, 0])\n",
    "    # cbar.set_ticks(ticks=[0, 1], labels=[round(dissimilarity_min, 4), round(dissimilarity_max, 4)])\n",
    "\n",
    "    # Entropy\n",
    "    # entropy_img = ax[4, 1].imshow(entropy_array, cmap='Reds')\n",
    "    # ax[4, 1].axis('off')\n",
    "    # ax[4, 1].set_title(\"Entropy\")\n",
    "    # cbar = fig.colorbar(entropy_img, ax=ax[4, 1])\n",
    "    # cbar.set_ticks(ticks=[0, 1], labels=[round(entropy_min, 4), round(entropy_max, 4)])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=Image.open('/Users/mussa/Desktop/PROJECTS/Data_edge/edgeEvalSimple.png')\n",
    "compute_features_plot(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
